{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "inference text_summarization_using_transformers.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9sC6bP1lICCL",
        "outputId": "90efaf9c-f3ba-40db-b7e0-e827a0e4a1ea"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r2WzQQXfIB_z"
      },
      "source": [
        "!ln -s /content/gdrive/My\\ Drive/ /mydrive"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_OptF8ZLIB9L"
      },
      "source": [
        "!cp -r  /mydrive/glove /content/glove/"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VyYy60IVD8bJ"
      },
      "source": [
        "!cp -r /mydrive/nlp_final_model/model.zip /content"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J7OWL9GPEKHT"
      },
      "source": [
        "!unzip /content/model.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kwQwhT0NtzuY"
      },
      "source": [
        "import re\n",
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd \n",
        "import unicodedata\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as krs\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy, CategoricalCrossentropy\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer \n",
        "from tensorflow.keras.optimizers.schedules import LearningRateSchedule\n",
        "import csv\n",
        "\n",
        "BUFFER_SIZE = 20000\n",
        "BATCH_SIZE = 64\n",
        "embedding_dim = 50"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A6qNa1_xuEUZ"
      },
      "source": [
        "!pip install kaggle\n",
        "!mkdir ~/.kaggle\n",
        "!cp /content/kaggle.json ~/.kaggle/kaggle.json"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "92Z7yiqIuESW",
        "outputId": "c58526c3-c1e3-44a8-d133-6fe51e590306"
      },
      "source": [
        "!kaggle datasets download -d shashichander009/inshorts-news-data"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /root/.kaggle/kaggle.json'\n",
            "Downloading inshorts-news-data.zip to /content\n",
            " 72% 9.00M/12.6M [00:00<00:00, 94.1MB/s]\n",
            "100% 12.6M/12.6M [00:00<00:00, 80.2MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ksj4__jiuEP4",
        "outputId": "292a8976-a338-453e-b1fd-d457ce850120"
      },
      "source": [
        "!unzip /content/inshorts-news-data.zip"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/inshorts-news-data.zip\n",
            "  inflating: Inshorts Cleaned Data.xlsx  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pG6fU-keDrc-"
      },
      "source": [
        ""
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "b-bv-rdcuENH",
        "outputId": "6e1a3b22-ea33-4cee-fc4a-8d3452f8e32f"
      },
      "source": [
        "data_unprocessed_news = pd.read_excel(\"/content/Inshorts Cleaned Data.xlsx\")\n",
        "data_unprocessed_news.head()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Headline</th>\n",
              "      <th>Short</th>\n",
              "      <th>Source</th>\n",
              "      <th>Time</th>\n",
              "      <th>Publish Date</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4 ex-bank officials booked for cheating bank o...</td>\n",
              "      <td>The CBI on Saturday booked four former officia...</td>\n",
              "      <td>The New Indian Express</td>\n",
              "      <td>09:25:00</td>\n",
              "      <td>2017-03-26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Supreme Court to go paperless in 6 months: CJI</td>\n",
              "      <td>Chief Justice JS Khehar has said the Supreme C...</td>\n",
              "      <td>Outlook</td>\n",
              "      <td>22:18:00</td>\n",
              "      <td>2017-03-25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>At least 3 killed, 30 injured in blast in Sylh...</td>\n",
              "      <td>At least three people were killed, including a...</td>\n",
              "      <td>Hindustan Times</td>\n",
              "      <td>23:39:00</td>\n",
              "      <td>2017-03-25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Why has Reliance been barred from trading in f...</td>\n",
              "      <td>Mukesh Ambani-led Reliance Industries (RIL) wa...</td>\n",
              "      <td>Livemint</td>\n",
              "      <td>23:08:00</td>\n",
              "      <td>2017-03-25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Was stopped from entering my own studio at Tim...</td>\n",
              "      <td>TV news anchor Arnab Goswami has said he was t...</td>\n",
              "      <td>YouTube</td>\n",
              "      <td>23:24:00</td>\n",
              "      <td>2017-03-25</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            Headline  ... Publish Date\n",
              "0  4 ex-bank officials booked for cheating bank o...  ...   2017-03-26\n",
              "1     Supreme Court to go paperless in 6 months: CJI  ...   2017-03-25\n",
              "2  At least 3 killed, 30 injured in blast in Sylh...  ...   2017-03-25\n",
              "3  Why has Reliance been barred from trading in f...  ...   2017-03-25\n",
              "4  Was stopped from entering my own studio at Tim...  ...   2017-03-25\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "YfF6721quEK1",
        "outputId": "8f6e073f-dba3-4024-9ee8-b366a8d9ee45"
      },
      "source": [
        "from sklearn.utils import shuffle\n",
        "data_unprocessed_news = shuffle(data_unprocessed_news,random_state=42)\n",
        "data_unprocessed_news.head()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Headline</th>\n",
              "      <th>Short</th>\n",
              "      <th>Source</th>\n",
              "      <th>Time</th>\n",
              "      <th>Publish Date</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2098</th>\n",
              "      <td>Gangster-turned-politician Mukhtar Ansari wins...</td>\n",
              "      <td>Gangster-turned-politician Mukhtar Ansari has ...</td>\n",
              "      <td>loading</td>\n",
              "      <td>18:58:00</td>\n",
              "      <td>2017-03-11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8760</th>\n",
              "      <td>Indira Gandhi only woman to have presented the...</td>\n",
              "      <td>Indira Gandhi has been the only woman till dat...</td>\n",
              "      <td>Livemint</td>\n",
              "      <td>16:59:00</td>\n",
              "      <td>2017-01-27</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42126</th>\n",
              "      <td>Sandler brings doppelgänger to film premiere</td>\n",
              "      <td>Actor Adam Sandler brought his 23-year-old dop...</td>\n",
              "      <td>Mashable</td>\n",
              "      <td>18:26:00</td>\n",
              "      <td>2016-05-22</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28632</th>\n",
              "      <td>Aadhaar cannot be mandatory, SC reminds govt</td>\n",
              "      <td>The Supreme Court recently reminded the Centre...</td>\n",
              "      <td>Twitter</td>\n",
              "      <td>19:31:00</td>\n",
              "      <td>2016-09-26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36240</th>\n",
              "      <td>Robots create architecture with carbon fibre</td>\n",
              "      <td>Researchers at the University of Stuttgart hav...</td>\n",
              "      <td>YouTube</td>\n",
              "      <td>21:46:00</td>\n",
              "      <td>2016-08-03</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                Headline  ... Publish Date\n",
              "2098   Gangster-turned-politician Mukhtar Ansari wins...  ...   2017-03-11\n",
              "8760   Indira Gandhi only woman to have presented the...  ...   2017-01-27\n",
              "42126       Sandler brings doppelgänger to film premiere  ...   2016-05-22\n",
              "28632       Aadhaar cannot be mandatory, SC reminds govt  ...   2016-09-26\n",
              "36240       Robots create architecture with carbon fibre  ...   2016-08-03\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8kNxVnihuEBm",
        "outputId": "060a3078-5c65-4b1e-a9b1-2a3eee85b304"
      },
      "source": [
        "summaries, longreview = pd.DataFrame(), pd.DataFrame()\n",
        "summaries['short'] = data_unprocessed_news['Headline']#[:data_to_use]\n",
        "longreview['long'] = data_unprocessed_news['Short']#[:data_to_use]\n",
        "(summaries.shape,longreview.shape)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((55104, 1), (55104, 1))"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "zlIEdmKfuD_I",
        "outputId": "2a8ef5d0-5b4c-4336-cd50-e713c6b7eb9d"
      },
      "source": [
        "summaries.head()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>short</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2098</th>\n",
              "      <td>Gangster-turned-politician Mukhtar Ansari wins...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8760</th>\n",
              "      <td>Indira Gandhi only woman to have presented the...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42126</th>\n",
              "      <td>Sandler brings doppelgänger to film premiere</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28632</th>\n",
              "      <td>Aadhaar cannot be mandatory, SC reminds govt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36240</th>\n",
              "      <td>Robots create architecture with carbon fibre</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   short\n",
              "2098   Gangster-turned-politician Mukhtar Ansari wins...\n",
              "8760   Indira Gandhi only woman to have presented the...\n",
              "42126       Sandler brings doppelgänger to film premiere\n",
              "28632       Aadhaar cannot be mandatory, SC reminds govt\n",
              "36240       Robots create architecture with carbon fibre"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "HGFzVs-luD8S",
        "outputId": "1c60311d-b218-46e8-d051-0f18965f9731"
      },
      "source": [
        "longreview.head()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>long</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2098</th>\n",
              "      <td>Gangster-turned-politician Mukhtar Ansari has ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8760</th>\n",
              "      <td>Indira Gandhi has been the only woman till dat...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42126</th>\n",
              "      <td>Actor Adam Sandler brought his 23-year-old dop...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28632</th>\n",
              "      <td>The Supreme Court recently reminded the Centre...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36240</th>\n",
              "      <td>Researchers at the University of Stuttgart hav...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                    long\n",
              "2098   Gangster-turned-politician Mukhtar Ansari has ...\n",
              "8760   Indira Gandhi has been the only woman till dat...\n",
              "42126  Actor Adam Sandler brought his 23-year-old dop...\n",
              "28632  The Supreme Court recently reminded the Centre...\n",
              "36240  Researchers at the University of Stuttgart hav..."
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "axciokFquD5_"
      },
      "source": [
        "def clean_words(sentence):\n",
        "    sentence = str(sentence).lower()\n",
        "    sentence = unicodedata.normalize('NFKD', sentence).encode('ascii', 'ignore').decode('utf-8', 'ignore') # for converting é to e and other accented chars\n",
        "    sentence = re.sub(r\"http\\S+\",\"\",sentence)\n",
        "    sentence = re.sub(r\"there's\", \"there is\", sentence)\n",
        "    sentence = re.sub(r\"i'm\", \"i am\", sentence)\n",
        "    sentence = re.sub(r\"he's\", \"he is\", sentence)\n",
        "    sentence = re.sub(r\"she's\", \"she is\", sentence)\n",
        "    sentence = re.sub(r\"it's\", \"it is\", sentence)\n",
        "    sentence = re.sub(r\"that's\", \"that is\", sentence)\n",
        "    sentence = re.sub(r\"what's\", \"that is\", sentence)\n",
        "    sentence = re.sub(r\"where's\", \"where is\", sentence)\n",
        "    sentence = re.sub(r\"how's\", \"how is\", sentence)\n",
        "    sentence = re.sub(r\"\\'ll\", \" will\", sentence)\n",
        "    sentence = re.sub(r\"\\'ve\", \" have\", sentence)\n",
        "    sentence = re.sub(r\"\\'re\", \" are\", sentence)\n",
        "    sentence = re.sub(r\"\\'d\", \" would\", sentence)\n",
        "    sentence = re.sub(r\"\\'re\", \" are\", sentence)\n",
        "    sentence = re.sub(r\"won't\", \"will not\", sentence)\n",
        "    sentence = re.sub(r\"can't\", \"cannot\", sentence)\n",
        "    sentence = re.sub(r\"n't\", \" not\", sentence)\n",
        "    sentence = re.sub(r\"n'\", \"ng\", sentence)\n",
        "    sentence = re.sub(r\"'bout\", \"about\", sentence)\n",
        "    sentence = re.sub(r\"'til\", \"until\", sentence)\n",
        "    sentence = re.sub(r\"\\\"\", \"\", sentence)\n",
        "    sentence = re.sub(r\"\\'\", \"\", sentence)\n",
        "    sentence = re.sub(r' s ', \"\",sentence)\n",
        "    sentence = re.sub(r\"&39\", \"\", sentence)\n",
        "    sentence = re.sub(r\"&34\", \"\", sentence) \n",
        "    sentence = re.sub(r\"[\\[\\]\\\\0-9()\\\"$#%/@;:<>{}`+=~|.!?,-]\", \"\", sentence)\n",
        "    sentence = re.sub(r\"&\", \"\", sentence)\n",
        "    sentence = re.sub(r\"\\\\n\", \"\", sentence)\n",
        "    sentence = sentence.strip()\n",
        "    return sentence"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "529Opbpkvarg"
      },
      "source": [
        "summaries['short'] = summaries['short'].apply(lambda x: clean_words(x))\n",
        "longreview['long'] = longreview['long'].apply(lambda x: clean_words(x))"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "50o5fQlOvc1n",
        "outputId": "d750d2db-e7df-48b7-cae3-275d194d1049"
      },
      "source": [
        "start_token, end_token = '<startseq>' , '<endseq>'\n",
        "summaries = summaries.apply(lambda x: start_token + ' ' + x + ' ' + end_token)\n",
        "summaries.head()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>short</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2098</th>\n",
              "      <td>&lt;startseq&gt; gangsterturnedpolitician mukhtar an...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8760</th>\n",
              "      <td>&lt;startseq&gt; indira gandhi only woman to have pr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42126</th>\n",
              "      <td>&lt;startseq&gt; sandler brings doppelganger to film...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28632</th>\n",
              "      <td>&lt;startseq&gt; aadhaar cannot be mandatory sc remi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36240</th>\n",
              "      <td>&lt;startseq&gt; robots create architecture with car...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   short\n",
              "2098   <startseq> gangsterturnedpolitician mukhtar an...\n",
              "8760   <startseq> indira gandhi only woman to have pr...\n",
              "42126  <startseq> sandler brings doppelganger to film...\n",
              "28632  <startseq> aadhaar cannot be mandatory sc remi...\n",
              "36240  <startseq> robots create architecture with car..."
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G_m7PUzwviNr",
        "outputId": "cbb29f67-3e4a-440a-c6a2-e2ca9703b030"
      },
      "source": [
        "val_split = 0.1\n",
        "# train validation split\n",
        "summaries_train = summaries[int(len(summaries)*val_split):]\n",
        "summaries_val = summaries[:int(len(summaries)*val_split)]\n",
        "longreview_train = longreview[int(len(summaries)*val_split):]\n",
        "longreview_val = longreview[:int(len(summaries)*val_split)]\n",
        "\n",
        "len(longreview_val),len(longreview_train)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5510, 49594)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LqSFlLBXvp4X"
      },
      "source": [
        "def max_length(shorts, longs, prct):\n",
        "    \n",
        "    length_longs = list(len(d.split()) for d in longs)\n",
        "    length_shorts = list(len(d.split()) for d in shorts)\n",
        "\n",
        "    print('percentile {} of length of news: {}'.format(prct,np.percentile(length_longs, prct)))\n",
        "    print('longest sentence: ', max(length_longs))\n",
        "    print()\n",
        "    print('percentile {} of length of summaries: {}'.format(prct,np.percentile(length_shorts, prct)))\n",
        "    print('longest sentence: ', max(length_shorts))\n",
        "    print()\n",
        "    return int(np.percentile(length_longs, prct)),int(np.percentile(length_shorts, prct))"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bYO2cu6XwJCG",
        "outputId": "887ba075-f956-405f-c9f2-d347a56768cd"
      },
      "source": [
        "max_len_news, max_len_summary= max_length(summaries_train['short'].to_list(), longreview_train['long'].to_list(), 90)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "percentile 90 of length of news: 60.0\n",
            "longest sentence:  66\n",
            "\n",
            "percentile 90 of length of summaries: 12.0\n",
            "longest sentence:  16\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eN-YXikRwI_j"
      },
      "source": [
        "def create_vocab(shorts, longs = None, minimum_repeat = 3):\n",
        "\n",
        "    all_captions = []\n",
        "    for s in shorts:\n",
        "        all_captions.append(s)\n",
        "\n",
        "    word_counts = {}\n",
        "    nsents = 0\n",
        "    for sent in all_captions:\n",
        "        nsents += 1\n",
        "        for w in sent.split(' '):\n",
        "            word_counts[w] = word_counts.get(w, 0) + 1\n",
        "\n",
        "    vocab = [w for w in word_counts if word_counts[w] >= minimum_repeat]\n",
        "    \n",
        "    vocab = list(set(vocab))\n",
        "    return vocab"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DqwnQ8LGwI89",
        "outputId": "038099f0-366d-4587-bd93-d70287d60c00"
      },
      "source": [
        "vocab_dec = create_vocab(summaries_train['short'].to_list(), minimum_repeat=5) # here we just use the words in vocabulary of summaries\n",
        "\n",
        "for v in vocab_dec:\n",
        "    if len(v) == 1 and v!='a' and v!='i':\n",
        "        vocab_dec.remove(v) \n",
        "        \n",
        "vocab_dec = sorted(vocab_dec)[1:] \n",
        "vocab_dec[:10]"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<endseq>',\n",
              " '<startseq>',\n",
              " 'a',\n",
              " 'aa',\n",
              " 'aadhaar',\n",
              " 'aadhaarbased',\n",
              " 'aadmi',\n",
              " 'aam',\n",
              " 'aamir',\n",
              " 'aamirs']"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qjm0VU18wzfZ",
        "outputId": "80b5a194-be33-4a0b-eb3e-73d2758973c8"
      },
      "source": [
        "vocab_enc = create_vocab(longreview_train['long'].to_list(), minimum_repeat=3) \n",
        "\n",
        "for v in vocab_enc:\n",
        "    if len(v) == 1 and v!='a' and v!='i':\n",
        "        vocab_enc.remove(v) \n",
        "        \n",
        "vocab_enc = sorted(vocab_enc)[1:] \n",
        "vocab_enc[:10]"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['a',\n",
              " 'a**holes',\n",
              " 'aa',\n",
              " 'aadat',\n",
              " 'aadhaar',\n",
              " 'aadhaarbased',\n",
              " 'aadhaarenabled',\n",
              " 'aadhaarlinked',\n",
              " 'aadhaars',\n",
              " 'aadhar']"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mxmK6sUrwzcy",
        "outputId": "d9a2ff0e-fcbc-4a73-84d7-3ae95b8219f9"
      },
      "source": [
        "oov_token = '<UNK>'\n",
        "filters = '!\"#$%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n' \n",
        "\n",
        "document_tokenizer = krs.preprocessing.text.Tokenizer(filters = filters,oov_token=oov_token)\n",
        "summary_tokenizer = krs.preprocessing.text.Tokenizer(filters = filters,oov_token=oov_token)\n",
        "\n",
        "document_tokenizer.fit_on_texts(vocab_enc)\n",
        "summary_tokenizer.fit_on_texts(vocab_dec)\n",
        "\n",
        "encoder_vocab_size = len(document_tokenizer.word_index) + 1 \n",
        "decoder_vocab_size = len(summary_tokenizer.word_index) + 1\n",
        "\n",
        "# vocab_size\n",
        "encoder_vocab_size, decoder_vocab_size"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(32421, 8900)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EU0EojpXwzak"
      },
      "source": [
        "ixtoword_enc = {} # index to word dic\n",
        "ixtoword_dec = {} # index to word dic\n",
        "\n",
        "wordtoix_enc = document_tokenizer.word_index \n",
        "ixtoword_enc[0] = '<PAD0>' \n",
        "ixtoword_dec[0] = '<PAD0>' \n",
        "\n",
        "for w in document_tokenizer.word_index:\n",
        "    ixtoword_enc[document_tokenizer.word_index[w]] = w\n",
        "\n",
        "wordtoix_dec = summary_tokenizer.word_index \n",
        "\n",
        "for w in summary_tokenizer.word_index:\n",
        "    ixtoword_dec[summary_tokenizer.word_index[w]] = w"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iyJjh0YiwzYv"
      },
      "source": [
        "inputs = document_tokenizer.texts_to_sequences(longreview_train['long'])\n",
        "targets = summary_tokenizer.texts_to_sequences(summaries_train['short'])\n",
        "\n",
        "inputs_val = document_tokenizer.texts_to_sequences(longreview_val['long'])\n",
        "targets_val = summary_tokenizer.texts_to_sequences(summaries_val['short'])"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yf11eOUFwzWj"
      },
      "source": [
        "inputs = krs.preprocessing.sequence.pad_sequences(inputs, maxlen=max_len_news, padding='post', truncating='post')\n",
        "targets = krs.preprocessing.sequence.pad_sequences(targets, maxlen=max_len_summary, padding='post', truncating='post')\n",
        "\n",
        "inputs_val = krs.preprocessing.sequence.pad_sequences(inputs_val, maxlen=max_len_news, padding='post', truncating='post')\n",
        "targets_val = krs.preprocessing.sequence.pad_sequences(targets_val, maxlen=max_len_summary, padding='post', truncating='post')"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r7JI38oswzUB"
      },
      "source": [
        "dataset = tf.data.Dataset.from_tensor_slices((inputs,targets)).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "dataset_val = tf.data.Dataset.from_tensor_slices((inputs_val,targets_val)).shuffle(BUFFER_SIZE).batch(BATCH_SIZE*2)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Shu93RYzwzRi"
      },
      "source": [
        "longreview_val.reset_index(inplace=True, drop=True)\n",
        "summaries_val.reset_index(inplace=True, drop=True)\n",
        "longreview_train.reset_index(inplace=True, drop=True)\n",
        "summaries_train.reset_index(inplace=True, drop=True)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FXPdnBqjwzPt"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def hist(history):\n",
        "    plt.title('Loss')\n",
        "\n",
        "    x= [i[0] for i in history['val']]\n",
        "    y=[i[1] for i in history['val']]\n",
        "    plt.plot(x,y,'x-')\n",
        "    \n",
        "    x= [i[0] for i in history['train']]\n",
        "    y=[i[1] for i in history['train']]    \n",
        "    plt.plot(x,y,'o-')\n",
        "\n",
        "    plt.legend(['validation','train'])\n",
        "    plt.show()\n",
        "    print('smallest val loss:', sorted(history['val'],key=lambda x: x[1])[0])"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-vwlLXn_SWBN",
        "outputId": "8a12771c-919a-48a2-e260-40d1114448d2"
      },
      "source": [
        "test_sen = ['Hello How are you ?', 'Hello', 'How are you', 'I am fine', 'Thank You']\n",
        "test_seq = document_tokenizer.texts_to_sequences(test_sen)\n",
        "test_seq"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[12707, 13161, 1691, 32245],\n",
              " [12707],\n",
              " [13161, 1691, 32245],\n",
              " [13341, 1066, 10434],\n",
              " [29024, 32245]]"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uPs3_krUSjjs",
        "outputId": "72b7eb86-3139-4310-d898-a0e02b7d0136"
      },
      "source": [
        "test_sen = ['Hello How are you ?', 'Hello', 'How are you', 'I am fine', 'Thank You']\n",
        "test_seq = summary_tokenizer.texts_to_sequences(test_sen)\n",
        "test_seq"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[3552, 3693, 475, 8857],\n",
              " [3552],\n",
              " [3693, 475, 8857],\n",
              " [3741, 311, 2950],\n",
              " [7971, 8857]]"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YkNHQoaGwzNK"
      },
      "source": [
        "def scaled_dot_product_attention(q, k, v, mask):\n",
        "    matmul_qk = tf.matmul(q, k, transpose_b=True)\n",
        "\n",
        "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
        "\n",
        "    if mask is not None:\n",
        "        scaled_attention_logits += (mask * -1e9)  \n",
        "\n",
        "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
        "\n",
        "    output = tf.matmul(attention_weights, v)\n",
        "    return output, attention_weights"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E6NgqvxawzHM"
      },
      "source": [
        "class MultiHeadAttention(krs.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "\n",
        "        assert d_model % self.num_heads == 0\n",
        "\n",
        "        self.depth = d_model // self.num_heads # The dimensions of Q, K, V are called depth\n",
        "\n",
        "        # the input of these 3 layers are the same: X\n",
        "        self.wq = krs.layers.Dense(d_model,kernel_regularizer=krs.regularizers.l2(l=lmbda_l2))\n",
        "        self.wk = krs.layers.Dense(d_model,kernel_regularizer=krs.regularizers.l2(l=lmbda_l2))\n",
        "        self.wv = krs.layers.Dense(d_model,kernel_regularizer=krs.regularizers.l2(l=lmbda_l2))\n",
        "\n",
        "        self.dense = tf.keras.layers.Dense(d_model,kernel_regularizer=krs.regularizers.l2(l=lmbda_l2))\n",
        "    \n",
        "    # reshape the Q,K,V \n",
        "    def split_heads(self, x, batch_size):\n",
        "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "    \n",
        "    def call(self, v, k, q, mask):\n",
        "        batch_size = tf.shape(q)[0]\n",
        "        \n",
        "        # learn the Q,K,V matrices (the layers' weightes)\n",
        "        q = self.wq(q)\n",
        "        k = self.wk(k)\n",
        "        v = self.wv(v)\n",
        "        \n",
        "        # reshape them\n",
        "        q = self.split_heads(q, batch_size)\n",
        "        k = self.split_heads(k, batch_size)\n",
        "        v = self.split_heads(v, batch_size)\n",
        "\n",
        "        scaled_attention, attention_weights = scaled_dot_product_attention(\n",
        "            q, k, v, mask)\n",
        "\n",
        "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
        "        \n",
        "        # the last dens layer expect one vector so we use concat\n",
        "        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.d_model))\n",
        "        output = self.dense(concat_attention)\n",
        "            \n",
        "        return output, attention_weights"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AlxIvaXXwzDg"
      },
      "source": [
        "def get_angles(position, i, d_model):\n",
        "    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
        "    return position * angle_rates\n",
        "\n",
        "def positional_encoding(position, d_model):\n",
        "    angle_rads = get_angles(\n",
        "        np.arange(position)[:, np.newaxis],\n",
        "        np.arange(d_model)[np.newaxis, :],\n",
        "        d_model\n",
        "        )\n",
        "\n",
        "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "    pos_encoding = angle_rads[np.newaxis, ...]\n",
        "\n",
        "    return tf.cast(pos_encoding, dtype=tf.float32)"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W92xTIMWwzA6"
      },
      "source": [
        "def make_embedding_layer(vocab_len, wordtoix, embedding_dim=200, glove=True, glove_path= '/content/glove'):\n",
        "    if glove == False:\n",
        "        print('Just a zero matrix loaded')\n",
        "        embedding_matrix = np.zeros((vocab_len, embedding_dim)) \n",
        "    else:\n",
        "        \n",
        "        print('Loading glove...')\n",
        "        glove_dir = glove_path\n",
        "        embeddings_index = {} \n",
        "        f = open(os.path.join(glove_dir, 'glove.6B.'+str(embedding_dim)+'d.txt'), encoding=\"utf-8\")\n",
        "        for line in f:\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            coefs = np.asarray(values[1:], dtype='float32')\n",
        "            embeddings_index[word] = coefs\n",
        "        f.close()\n",
        "        \n",
        "        embedding_matrix = np.zeros((vocab_len, embedding_dim)) # to import as weights for Keras Embedding layer\n",
        "        for word, i in wordtoix.items():\n",
        "            embedding_vector = embeddings_index.get(word)\n",
        "            if embedding_vector is not None:\n",
        "                # Words not found in the embedding index will be all zeros\n",
        "                embedding_matrix[i] = embedding_vector\n",
        "        \n",
        "        print(\"GloVe \",embedding_dim, ' loaded!')\n",
        "\n",
        "    embedding_layer = Embedding(vocab_len, embedding_dim, mask_zero=True, trainable=False) \n",
        "    embedding_layer.build((None,))\n",
        "    embedding_layer.set_weights([embedding_matrix])\n",
        "    return embedding_layer"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UCeJRTgXwy-4",
        "outputId": "de80c130-2a61-46fb-cbce-8bc7fbdcaef1"
      },
      "source": [
        "embeddings_encoder = make_embedding_layer(encoder_vocab_size, wordtoix_enc, embedding_dim=embedding_dim, glove=True)\n",
        "embeddings_decoder = make_embedding_layer(decoder_vocab_size, wordtoix_dec, embedding_dim=embedding_dim, glove=True)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading glove...\n",
            "GloVe  50  loaded!\n",
            "Loading glove...\n",
            "GloVe  50  loaded!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YR5i2qegwy8M"
      },
      "source": [
        "# hyper-params\n",
        "init_lr = 1e-3\n",
        "lmbda_l2 = 0.1\n",
        "d_out_rate = 0.1 # tested 0.4, 0.3, 0.1 values this 0.1 seems to be the best\n",
        "num_layers = 4 # chaged from 4 to 5 to learn better\n",
        "d_model = embedding_dim # d_model is the representation dimension or embedding dimension of a word (usually in the range 128–512)\n",
        "dff = 512 # number of neurons in feed forward network\n",
        "num_heads = 5 # first it was 8 i chenged it to 10 to use embd =300d"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_G8OxXO752n7"
      },
      "source": [
        "def point_wise_feed_forward_network(d_model, dff):\n",
        "    return krs.Sequential([\n",
        "        krs.layers.Dense(dff, activation='relu',kernel_regularizer=krs.regularizers.l2(l=lmbda_l2)),\n",
        "        krs.layers.Dense(d_model,kernel_regularizer=krs.regularizers.l2(l=lmbda_l2))\n",
        "    ])"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7DSKrS3H52l5"
      },
      "source": [
        "class EncoderLayer(krs.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, dff, rate=d_out_rate):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "\n",
        "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
        "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "        self.layernorm1 = krs.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = krs.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.dropout1 = krs.layers.Dropout(rate)\n",
        "        self.dropout2 = krs.layers.Dropout(rate)\n",
        "   \n",
        "    # it has 1 layer of multi-headed attention\n",
        "    def call(self, x, training, mask):\n",
        "        attn_output, _ = self.mha(x, x, x, mask)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(x + attn_output)\n",
        "\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        out2 = self.layernorm2(out1 + ffn_output)\n",
        "\n",
        "        return out2"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQE1cLO452je"
      },
      "source": [
        "class DecoderLayer(krs.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, dff, rate=d_out_rate):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "\n",
        "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
        "        self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
        "\n",
        "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "        self.layernorm1 = krs.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = krs.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm3 = krs.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.dropout1 = krs.layers.Dropout(rate)\n",
        "        self.dropout2 = krs.layers.Dropout(rate)\n",
        "        self.dropout3 = krs.layers.Dropout(rate)\n",
        "    \n",
        "    # it has 2 layers of multi-headed attention\n",
        "    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
        "        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)\n",
        "        attn1 = self.dropout1(attn1, training=training)\n",
        "        out1 = self.layernorm1(attn1 + x)\n",
        "\n",
        "        attn2, attn_weights_block2 = self.mha2(enc_output, enc_output, out1, padding_mask)\n",
        "        attn2 = self.dropout2(attn2, training=training)\n",
        "        out2 = self.layernorm2(attn2 + out1)\n",
        "\n",
        "        ffn_output = self.ffn(out2)\n",
        "        ffn_output = self.dropout3(ffn_output, training=training)\n",
        "        out3 = self.layernorm3(ffn_output + out2)\n",
        "\n",
        "        return out3, attn_weights_block1, attn_weights_block2"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TDbBm3sm52hG"
      },
      "source": [
        "class Encoder(krs.layers.Layer):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, maximum_position_encoding, rate=d_out_rate):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = embeddings_encoder\n",
        "        self.pos_encoding = positional_encoding(maximum_position_encoding, self.d_model)\n",
        "\n",
        "        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n",
        "\n",
        "        self.dropout = krs.layers.Dropout(rate)\n",
        "        self.dropout_embd = krs.layers.Dropout(rate)\n",
        "        \n",
        "    def call(self, x, training, mask):\n",
        "        seq_len = tf.shape(x)[1]\n",
        "\n",
        "        x = self.embedding(x)\n",
        "        x = self.dropout_embd(x, training=training) # dropout added to encoder input changed from nothing to this\n",
        "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "        x = self.dropout(x, training=training)\n",
        "    \n",
        "        for i in range(self.num_layers):\n",
        "            x = self.enc_layers[i](x, training, mask)\n",
        "    \n",
        "        return x"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yXnvnigy52dw"
      },
      "source": [
        "class Decoder(krs.layers.Layer):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size, maximum_position_encoding, rate=d_out_rate):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = embeddings_decoder\n",
        "        self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
        "\n",
        "        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)] # a list of decoder layers\n",
        "        self.dropout = krs.layers.Dropout(rate)\n",
        "    \n",
        "    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
        "        seq_len = tf.shape(x)[1]\n",
        "        attention_weights = {}\n",
        "\n",
        "        x = self.embedding(x)\n",
        "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "        x = self.dropout(x, training=training)\n",
        "\n",
        "        for i in range(self.num_layers):\n",
        "            x, block1, block2 = self.dec_layers[i](x, enc_output, training, look_ahead_mask, padding_mask) # enc_output is fed into it\n",
        "\n",
        "            attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n",
        "            attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n",
        "    \n",
        "        return x, attention_weights"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wOQcwIp352ai"
      },
      "source": [
        "# class Transformer(krs.Model):\n",
        "#   def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,target_vocab_size, pe_input, pe_target, rate=d_out_rate):\n",
        "#     super(Transformer, self).__init__()\n",
        "#     self.encoder = Encoder(num_layers, d_model, num_heads, dff, input_vocab_size, pe_input, rate)\n",
        "#     self.decoder = Decoder(num_layers, d_model, num_heads, dff, target_vocab_size, pe_target, rate)\n",
        "#     self.final_layer = krs.layers.Dense(target_vocab_size, kernel_regularizer=krs.regularizers.l2(l=lmbda_l2))        \n",
        "\n",
        "#   def call(self, inp, tar, training, enc_padding_mask, look_ahead_mask, dec_padding_mask):\n",
        "#     enc_output = self.encoder(inp, training, enc_padding_mask)  \n",
        "#     dec_output, attention_weights = self.decoder(tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
        "#     final_output = self.final_layer(dec_output)\n",
        "#     return final_output, attention_weights"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a3D4sxiLRV9v"
      },
      "source": [
        "class Transformer(krs.Model):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
        "                                     target_vocab_size, pe_input, pe_target, rate=d_out_rate):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.encoder = Encoder(num_layers, d_model, num_heads, dff, input_vocab_size, pe_input, rate)\n",
        "        self.decoder = Decoder(num_layers, d_model, num_heads, dff, target_vocab_size, pe_target, rate)\n",
        "        self.final_layer = krs.layers.Dense(target_vocab_size, kernel_regularizer=krs.regularizers.l2(l=lmbda_l2))\n",
        "        \n",
        "        \n",
        "    # training argument is used in dropout inputs\n",
        "    def call(self, inp, tar, training, enc_padding_mask, look_ahead_mask, dec_padding_mask):\n",
        "        enc_output = self.encoder(inp, training, enc_padding_mask)\n",
        "        dec_output, attention_weights = self.decoder(tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
        "        final_output = self.final_layer(dec_output)\n",
        "        return final_output, attention_weights"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aiKYKB9C52X4"
      },
      "source": [
        "def create_padding_mask(seq):\n",
        "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
        "    return seq[:, tf.newaxis, tf.newaxis, :]\n",
        "\n",
        "\n",
        "def create_look_ahead_mask(size):\n",
        "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
        "    return mask"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8rVBLdtnwy5l"
      },
      "source": [
        "def create_masks(inp, tar):\n",
        "    enc_padding_mask = create_padding_mask(inp)\n",
        "    dec_padding_mask = create_padding_mask(inp)\n",
        "\n",
        "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
        "    dec_target_padding_mask = create_padding_mask(tar)\n",
        "        \n",
        "    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
        "  \n",
        "    return enc_padding_mask, combined_mask, dec_padding_mask"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sCpkddoRhji1"
      },
      "source": [
        "transformer = Transformer(\n",
        "    num_layers, \n",
        "    d_model, \n",
        "    num_heads, \n",
        "    dff,\n",
        "    encoder_vocab_size, \n",
        "    decoder_vocab_size, \n",
        "    pe_input=max_len_news,\n",
        "    pe_target=max_len_summary,\n",
        ")"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bFV1clKJRJG3"
      },
      "source": [
        ""
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5JVol6IySN6C"
      },
      "source": [
        "lr_schedule = krs.optimizers.schedules.ExponentialDecay(initial_learning_rate=init_lr, decay_steps=4000, decay_rate=0.95)"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SF-wTidUSN4h"
      },
      "source": [
        "optimizer2 = Adam(lr_schedule , beta_1=0.9, beta_2=0.98, epsilon=1e-9) # changed to init\n",
        "loss_object = SparseCategoricalCrossentropy(from_logits=True, reduction='none') # added softmax changed from_logits to false"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C2kXHiY-SN27"
      },
      "source": [
        "def loss_function(real,pred,l2=False):\n",
        "  \n",
        "  if l2:\n",
        "    lambda_ = 0.0001\n",
        "    l2_norms = [tf.nn.l2_loss(v) for v in transformer.trainable_variables]\n",
        "    l2_norm = tf.reduce_sum(l2_norms)\n",
        "    l2_value = lambda_ * l2_norm\n",
        "    loss_ = loss_object(real, pred) + l2_value\n",
        "  \n",
        "  else:\n",
        "    loss_ = loss_object(real,pred)\n",
        "  \n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  mask = tf.cast(mask,dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "\n",
        "  return tf.reduce_sum(loss_)/tf.reduce_sum(mask)"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nJQk_gbpSN1J"
      },
      "source": [
        "checkpoint_path4 =\"checkpoints4\"\n",
        "ckpt4 = tf.train.Checkpoint(transformer=transformer, optimizer=optimizer2)\n",
        "ckpt_manager4 = tf.train.CheckpointManager(ckpt4, checkpoint_path4, max_to_keep=100)"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pp9MNN_5SNxW"
      },
      "source": [
        "def evaluate(input_document):\n",
        "    input_document = document_tokenizer.texts_to_sequences([input_document])\n",
        "    input_document = krs.preprocessing.sequence.pad_sequences(input_document, maxlen=max_len_news, padding='post', truncating='post')\n",
        "    \n",
        "    encoder_input = tf.expand_dims(input_document[0], 0)\n",
        "\n",
        "    decoder_input = [summary_tokenizer.word_index[start_token]]\n",
        "    output = tf.expand_dims(decoder_input, 0)\n",
        "    \n",
        "    for i in range(max_len_summary):\n",
        "        enc_padding_mask, combined_mask, dec_padding_mask = create_masks(encoder_input, output)\n",
        "\n",
        "        predictions, attention_weights = transformer(\n",
        "            encoder_input, \n",
        "            output,\n",
        "            False,\n",
        "            enc_padding_mask,\n",
        "            combined_mask,\n",
        "            dec_padding_mask\n",
        "        )\n",
        "\n",
        "        predictions = predictions[: ,-1:, :]\n",
        "        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
        "        # stop prediciting if it reached end_token\n",
        "        if predicted_id == summary_tokenizer.word_index[end_token]:\n",
        "            return tf.squeeze(output, axis=0), attention_weights\n",
        "\n",
        "        output = tf.concat([output, predicted_id], axis=-1)\n",
        "    return tf.squeeze(output, axis=0), attention_weights"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Flvwe6FSNuF"
      },
      "source": [
        "def summarize(input_document):\n",
        "    summarized = evaluate(input_document=input_document)[0].numpy()\n",
        "    summarized = np.expand_dims(summarized[1:], 0)  \n",
        "    return summary_tokenizer.sequences_to_texts(summarized)[0]  "
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c5GR-m6VEgnj",
        "outputId": "76852800-da7a-4dc6-b5ba-528e5211aa67",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "ckpt4.restore(ckpt_manager4.latest_checkpoint)\n",
        "if ckpt_manager4.latest_checkpoint:\n",
        "    print(\"Restored from {}\".format(ckpt_manager4.latest_checkpoint))"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Restored from checkpoints4/ckpt-16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ckpt4.restore(\"checkpoints4/ckpt-15\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q8prXH0wkj8M",
        "outputId": "f3894f99-65a1-4508-caf0-a39281bd5d4f"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fddc44f30d0>"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9jHE_P1qEgl6"
      },
      "source": [
        ""
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0oQ83vsEgka",
        "outputId": "72bcaf4a-00bb-445a-ab8f-c3a3c546610c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(summarize(clean_words(\"Finding software solutions to professional technical issues will decide how well we perform in the days to come. Technology for sure is the driver today. In pursuit of such solutions India Police Hackathon 2019 is being organised by Karnataka State Police and co-hosted by RV College of Engineering, Bengaluru, on their campus. IEEE is the Knowledge Partner providing most of the Mentors and Jury. India Police Hackathon 2019 is scheduled to be on 16th and 17th November, a 36-hour Hackathon. It is an Open Hackathon with No Registration Fee and Online Elimination Round. Around 25 teams of 3 to 5 each will participate in the Hackathon, based on the Elimination Round.\")))"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "karnataka police to play student to teach students\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "s9fpHm7TrNuy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "UX4BmcdPrNr3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "oEnXS3a5rNo2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "abmQeic3Eghu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b536845-3fcb-48a4-ae19-319e091040c2"
      },
      "source": [
        "for i in range(50):\n",
        "  rand = np.random.randint(len(longreview_val))\n",
        "  print( longreview_val['long'][rand] )\n",
        "  print( summarize(clean_words(longreview_val['long'][rand] )))\n",
        "  print(\"*\"*50)\n",
        "  print(\"\\n\")"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ecommerce giant amazon has filed three new lawsuits against sellers who buy fake reviews for products listed on its platform the lawsuits claim that over  of reviews on products of two of these sellers are fake since  amazon has sued more than  defendants who offered to post fake reviews on its site in exchange for compensation\n",
            "amazon sued for fake fake fake fake products\n",
            "**************************************************\n",
            "\n",
            "\n",
            "the obama administration on wednesday proposed an  million aid package for pakistan which includes  million for purchasing military hardware the military aid is aimed at helping pakistan secure nuclear weapons fight terrorists and improve ties with india us secretary of state john kerry said pakistan lies at the heart of the us counterterrorism strategy\n",
            "us proposes mn for defence aid to pak\n",
            "**************************************************\n",
            "\n",
            "\n",
            "xiaomi on thursday launched its mi max smartphone in india at a starting price of  the handset runs on android  marshmallow and features a inch display  gb ram  gb memory  mp rear camera and qualcomm snapdragon  processor scheduled for a flash sale on july  it will come to stores on july\n",
            "xiaomi launches <UNK> smartphone at\n",
            "**************************************************\n",
            "\n",
            "\n",
            "ms dhoni broke sachin tendulkars record of most odi sixes by an indian hitting his th in the third odi against new zealand on sunday sachin had hit  sixes in  odis while dhoni completed  sixes in his st match shahid afridi holds the record for the most sixes in odis with  sixes in  matches\n",
            "ms dhoni breaks record for most odi wickets in odi\n",
            "**************************************************\n",
            "\n",
            "\n",
            "indian racer narain karthikeyan posted his first podium finish of the year at the super formula championship after finishing third at the okayama international circuit in mimasaka japan on saturday with the result karthikeyan is now placed th position in the championship standings i am confident i will be right up there fighting for podiums said karthikeyan after the finish\n",
            "<UNK> <UNK> wins japans st <UNK> title\n",
            "**************************************************\n",
            "\n",
            "\n",
            "the yearlong simhastha kumbh mela in nashik ended on thursday with maharashtra chief minister devendra fadnavis and bjp president amit shah attending the concluding event the mela had begun in trimbakeshwar and nashik on july  last year with a flaghoisting ceremony the ancient gangagodavari temple will now be closed till the next kumbh mela to be held in\n",
            "<UNK> <UNK> <UNK> <UNK> <UNK> in maharashtra\n",
            "**************************************************\n",
            "\n",
            "\n",
            "mitchell starc smashed seven sixes in the first innings against pakistan thereby breaking the record for most sixes hit in a test innings at the melbourne cricket ground mcg is considered to be one of the biggest grounds with the longest boundary being over  meters starc overtook andrew symonds who had hit six sixes against south africa in\n",
            "<UNK> records highest ever ever ever ever in tests\n",
            "**************************************************\n",
            "\n",
            "\n",
            "india contributed nearly twenty five lakh soldiers to the british war effort in the second world war making it the largest ever volunteer force in the world starting with nearly  lakh soldiers in  and rising to over  lakh by  the army fought in asia africa and europe notably world war ii ended on september\n",
            "india has the largest war in war of the world war\n",
            "**************************************************\n",
            "\n",
            "\n",
            "india reached  at the end of the second day of the third test against england at mohali on sunday the indian innings was driven by cheteshwar pujaras  and virat kohlis  while englands adil rashid picked up three wickets allrounders ravichandran ashwin and ravindra jadeja remained not out on  and  at stumps\n",
            "india end day at end day at\n",
            "**************************************************\n",
            "\n",
            "\n",
            "maharashtra police have arrested a paper scrap merchant in thane for allegedly possessing fake notes of the demonetised currency and depositing them in a bank reports said on sunday the yearold named pankaj kori reportedly went to a public sector bank for depositing cash amounting to  lakh in the defunct  notes out of which  notes were fake\n",
            "fake notes arrested for depositing lakh in mumbai\n",
            "**************************************************\n",
            "\n",
            "\n",
            "billionaire warren buffett and firm berkshire hathaways board have opposed a proposal to disclose its political contributions it said during past years political contributions of berkshire subsidiaries have been less than  million per year further two other proposals have been made for berkshire to divest in firms that produce fossil fuels and set targets for cutting methane emissions\n",
            "buffett rejects <UNK> <UNK> <UNK>\n",
            "**************************************************\n",
            "\n",
            "\n",
            "hundreds of dogs and their owners took part in the sixth annual san perrestre dog race in madrid spain on friday the race was held to raise awareness about pet adoption and abandoned animals it was organised by el refugio animal home five of the dogs competing in the race have been put up for adoption\n",
            "spain held to <UNK> <UNK> for dogs\n",
            "**************************************************\n",
            "\n",
            "\n",
            "industrialist ratan tata has termed demonetisation as one of the most important economic reforms adding that the bold move needs the nations support the governments fight against black money needs support and cooperation of all like minded citizens of india he said earlier tata raised concerns over demonetisation asking the government to consider special relief measures for the poor\n",
            "<UNK> <UNK> <UNK> <UNK> <UNK> tata\n",
            "**************************************************\n",
            "\n",
            "\n",
            "turkey russia and iran have reached agreement on forming a threeparty mechanism to monitor ceasefire violations in syria concluding the talks held in kazakhstans capital astana notably iran and russia support the syrian government while turkey supports rebels opposing the government syrian rebels also took part in the talks which were indirect negotiations with the syrian government\n",
            "russia syria syria agree to <UNK> syria ceasefire\n",
            "**************************************************\n",
            "\n",
            "\n",
            "the reserve bank of india on monday raised the withdrawal limit from atms to  per day from the existing  per day per card with immediate effect further the limit on withdrawal from current accounts has also been enhanced to  lakh per week from the existing limit of  per week\n",
            "rbi raises <UNK> limit to lakh from <UNK>\n",
            "**************************************************\n",
            "\n",
            "\n",
            "bjp mp subramanian swamy on friday blamed rbi governor raghuram rajan for a dip in the growth rate of micro small and medium enterprises msmes as you can see the msme growth has sharply declined in one year in  data seen sharper decline r effect he tweeted while sharing the snapshot of a report on msmes performance since\n",
            "swamy slams rbi over <UNK> rate rate\n",
            "**************************************************\n",
            "\n",
            "\n",
            "kenyan mp mishi mboko has asked country women to withhold sex from their husbands until they register as voters for the upcoming presidential elections mboko said sex was a powerful weapon and would encourage reluctant men to rush to register as voters the deadline for voting registration is february  with the national elections scheduled to take place in august\n",
            "<UNK> asks women to <UNK> women to vote\n",
            "**************************************************\n",
            "\n",
            "\n",
            "more than  suspected drug users or dealers reportedly have been killed in the philippines after the election of president rodrigo duterte in may with promises of waging a war on illegal drugs this comes as human rights groups sent joint letters calling on un drug control bodies to publicly condemn these atrocities in the philippines\n",
            "philippines to kill drug war against philippines\n",
            "**************************************************\n",
            "\n",
            "\n",
            "barcelonabased jaume almera institute of earth sciences located near the camp nou measured a  tremor on the richter scale after sergi robertos goal which sealed barcelonas historic comeback win against psg in the champions league notably humans are able to typically detect earthquakes which measure  or greater on the richter scale\n",
            "<UNK> <UNK> <UNK> <UNK> <UNK>\n",
            "**************************************************\n",
            "\n",
            "\n",
            "dutch photojournalist jeroen oerlemans was shot dead by an islamic state sniper while oerlemans had been covering a government offensive against isis in libyas sirte on sunday the yearold was shot in the chest while he was out with a team clearing mines in the groups last libyan stronghold earlier oerlemans was held hostage by isis but was later released\n",
            "isis <UNK> shot dead in libya\n",
            "**************************************************\n",
            "\n",
            "\n",
            "a facebook post referring to us first lady michelle obama as an ape in heels has prompted social media outrage pamela taylor who works at a charity posted the remarks and beverly whaling the mayor of a town in us state of west virginia commented this just made my day taylor has been sacked from her job following the post\n",
            "us mayor <UNK> <UNK> as <UNK> <UNK>\n",
            "**************************************************\n",
            "\n",
            "\n",
            "american ecommerce major amazon is planning to launch its own line of bras in the us starting from  a piece the bras would be available in the country within the next few weeks amazon has already launched its own line of bras at similar prices in europe under the brand name iris amp lilly\n",
            "amazon to launch its own <UNK> <UNK>\n",
            "**************************************************\n",
            "\n",
            "\n",
            "bollywood producer abis rizvi was among the  people who were killed in the nightclub terror attack in istanbul rizvi son of former rajya sabha mp akhtar hassan rizvi had last produced and cowritten roar tigers of the sundarbans in  fashion designer khushi shah was the other indian national who was killed in the attack\n",
            "<UNK> son killed in <UNK> attack\n",
            "**************************************************\n",
            "\n",
            "\n",
            "congress mp hanumantha rao on wednesday denied making any antinational elements references in his letter to the hrd ministry and blamed union minister smriti irani for unnecessarily linking him to the hyderabad dalit scholar suicide rao said his letter only mentioned on campus suicides corruption caste discrimination and irregularities and that the hrd had not responded to it\n",
            "irani denies suicide over dalit suicide\n",
            "**************************************************\n",
            "\n",
            "\n",
            "rishi kapoor has tweeted that he is happy to accept birthday messages for his son ranbir kapoor who turned  on wednesday as ranbir is not on social media happy to accept this time all your greetings blessings love and hate for ranbir tweeted rishi however the picture of the grumpy cat meme he tweeted read im not ranbirs mailbox\n",
            "ranbir kapoor denies his birthday on his birthday\n",
            "**************************************************\n",
            "\n",
            "\n",
            "the hyderabad bicycling club the hyderabad metro rail and the un habitat on thursday entered into a tripartite agreement to set up about  bike stations across the city about  bicycles including ebikes will be made available at these stations further  of the bike stations will be situated at metro stations to provide passengers with lastmile connectivity\n",
            "hyderabad metro station to get <UNK> <UNK>\n",
            "**************************************************\n",
            "\n",
            "\n",
            "american financial services company morgan stanley has revealed several taxreporting errors were made by its brokerage business from  to  for which it has set aside  crore the amount will be used to reimburse the customers who overpaid taxes due to taxing errors morgan stanley is also looking to resolve any client tax underpayments\n",
            "morgan stanley makes crore tax to <UNK> tax evasion\n",
            "**************************************************\n",
            "\n",
            "\n",
            "as many as  naxals including  women surrendered before the police in chhattisgarhs sukma district according to reports this is the biggest naxal surrender in the bastar region to date police said some of the surrendered naxals have warrants pending and carry a reward under chhattisgarhs rehabilitation programme all  naxals were given  as immediate compensation\n",
            "naxals surrender in chhattisgarh\n",
            "**************************************************\n",
            "\n",
            "\n",
            "durex condoms have been banned by russia after the brand ran into registrationrelated issues in the country they durex condoms are not registered in the proper manner an official said according to reports durex accounts for onefourth of the condoms sold in russia however officials assured that there would be no shortage of condoms in the country\n",
            "russian <UNK> banned for <UNK> drugs\n",
            "**************************************************\n",
            "\n",
            "\n",
            "niraj kumar a close aide of selfstyled godman asaram has been booked under the arms act said the police on saturday kumar is accused of killing a witness in a rape case filed against asaram the police have seized a pistol on the basis of information gathered from him following which a warrant was issued on his name\n",
            "<UNK> <UNK> <UNK> arrested for murder\n",
            "**************************************************\n",
            "\n",
            "\n",
            "british antigarbage startup littergram has been asked by instagrams lawyers to change its name as the use of gram could undermine instagrams brand littergram lets users share photos of garbage geotag its location and report it to the local council the use of photography and social aspects of the app have also been argued as being similar to instagram\n",
            "<UNK> asks users to use <UNK> to use fake <UNK>\n",
            "**************************************************\n",
            "\n",
            "\n",
            "videos which show european nations including switzerland and germany mocking us president donald trump have been released online a video featuring switzerland says we also treat our women badly we didnt let them vote until we grabbed them by the civil rights while a video featuring germany says we built a great german wall andmade the russians pay for it\n",
            "germany releases video of trump <UNK>\n",
            "**************************************************\n",
            "\n",
            "\n",
            "a gelato store in the australian city of sydney serves colourful roseshaped ice creams the store manager of icreamy pat says we sculpt each flower petal by petal each flower gelato contains about  petals the ice cream flavours include caramel choco chip peanut butter pop corn and thai milk tea\n",
            "australian town has a <UNK> <UNK>\n",
            "**************************************************\n",
            "\n",
            "\n",
            "a fiveminute  virtual reality video explores what happens inside the human body and further shows how processes like respiration and digestion take place the video also takes the viewer along the bloodstream in order to explain how the body functions interestingly it also shows how the eye actually sees an inverted image and how the brain produces neurotransmitters\n",
            "video explains why we do we take <UNK>\n",
            "**************************************************\n",
            "\n",
            "\n",
            "the akhilesh yadavled government is reportedly exploring possibilities to start the up governmentrun tv news channel the centre however has stated that the state governments and assemblies would not be given such permission as per the report meanwhile the sp government has argued that if private and government institutions are granted such permission then why not state governments\n",
            "govt to start <UNK> tv report\n",
            "**************************************************\n",
            "\n",
            "\n",
            "jammu and kashmir government on friday decided to conduct the board examinations for class x and xii twice  giving the students a choice to appear either in november or march the decision was taken after complaints of noncompletion of syllabus by a section of students no concessions however will be offered to those appearing for the exams in march\n",
            "jampk govt to <UNK> exam exams for class exam\n",
            "**************************************************\n",
            "\n",
            "\n",
            "amsterdam will test selfdriving boats in the citys canals in a fiveyear pilot program starting next year besides carrying people and goods the selfdriving boat fleet will also track environmental conditions and comb the canals for waste floating stages or bridges that can be assembled and disassembled in hours will also be introduced in the city\n",
            "<UNK> to test <UNK> water in switzerland\n",
            "**************************************************\n",
            "\n",
            "\n",
            "reacting to commentator ian chappell saying that south african pacer kagiso rabada hails from a village a user tweeted if any village needs an idiot then ianchappell is applying ausvsa chuffed kagiso rabada and i come from the same village its a pretty big one called johannesburg savsaus tweeted another user\n",
            "<UNK> <UNK> <UNK> <UNK> <UNK> tweets user\n",
            "**************************************************\n",
            "\n",
            "\n",
            "according to a theory eyelids involuntarily shut during a sneeze to prevent the germs from entering and aggravating the eyes another theory suggests that muscle contractions induced by sneezing result in reflexive closing of the eyes these muscle movements also cause some people to shed tears while sneezing\n",
            "why do we <UNK> <UNK>\n",
            "**************************************************\n",
            "\n",
            "\n",
            "an emirates flight from oman to dubai was cancelled on sunday after a snake was detected in the cargo hold of the airplane by the baggage handlers the snake was found before any of the passengers boarded the plane the airplane later continued its journey after being thoroughly searched\n",
            "<UNK> flight lands from italy flight\n",
            "**************************************************\n",
            "\n",
            "\n",
            "congress leaders in uttarakhand on monday moved the nainital high court against the imposition of presidents rule in the state the presidents rule was imposed just ahead of a crucial vote in uttarakhand assembly where cm harish rawat had to show majority support for his congressled government this comes after a sting video allegedly showed rawat talking about bribing lawmakers\n",
            "ukhand ukhand <UNK> cong congress on ukhand floor test\n",
            "**************************************************\n",
            "\n",
            "\n",
            "actor arjun rampal has said that he is really sad that award shows are like a reality show i remember when i came into this industry there were such credible award shows and to win them was such a high said arjun he added all of this makes my national award even so much more special\n",
            "i have been a award of my <UNK> i was a award\n",
            "**************************************************\n",
            "\n",
            "\n",
            "xiaomi on thursday launched its mi max smartphone in india at a starting price of  the handset runs on android  marshmallow and features a inch display  gb ram  gb memory  mp rear camera and qualcomm snapdragon  processor scheduled for a flash sale on july  it will come to stores on july\n",
            "xiaomi launches <UNK> smartphone at\n",
            "**************************************************\n",
            "\n",
            "\n",
            "a fan threw a bra at singer rihanna while she was performing at a concert in copenhagen for her anti world tour the singer picked up the bra and remarked that it was too big for her breasts her current tour is for her eighth studio album anti which will end with her final concert in abu dhabi in december\n",
            "<UNK> <UNK> <UNK> at <UNK> concert at concert\n",
            "**************************************************\n",
            "\n",
            "\n",
            "the himachal pradesh government has launched a  crore project for skill development under which multipurpose training centres will be set up in rural areas the training to be given by both government and private associations will be accessible to all people aged between  including bpl families individuals will be trained in tourism agriculture and banking sectors among others\n",
            "himachal govt launches cr to train projects\n",
            "**************************************************\n",
            "\n",
            "\n",
            "the daughterinlaw of bsp rajya sabha mp narendra kashyap was on wednesday found dead inside their house in ghaziabad the police said the family members rushed a profusely bleeding himanshi kashyap  with a gunshot injury on her head to a hospital where she was declared brought dead her husband sagar told the police that she had committed suicide\n",
            "up mp <UNK> <UNK> death toll rises to\n",
            "**************************************************\n",
            "\n",
            "\n",
            "manchester city suffered a setback to their title challenge as they were held to a goalless draw by norwich city on saturday city came close to scoring when sergio aguero took a snapshot but it was saved by john ruddy with this draw city is now at the fourth spot with  points nine behind league leaders leicester city\n",
            "man city held to draw by west bengal city\n",
            "**************************************************\n",
            "\n",
            "\n",
            "on apple ceo tim cooks maiden visit to india the company today announced to set up its first ios app design and development accelerator in bengaluru in early  the accelerator will support developers to create mobile apps for the ios platform apple experts will lead briefings at the centre and provide oneonone app reviews for developers every week\n",
            "apple to launch first ios app for india\n",
            "**************************************************\n",
            "\n",
            "\n",
            "wayne allwine the man who served as the voice of disneys character mickey mouse for over  years was married to the woman who currently voices minnie mouse russi taylor they were married from  to  until allwine passed away due to complications from diabetes notably russi is considered to be the longestrunning voiceover artist for minnie mouse\n",
            "<UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
            "**************************************************\n",
            "\n",
            "\n",
            "an organisation of kashmiri pandits on monday demanded that members of the community who migrated from the valley be declared internally displaced people the demand which comes on the international refugees day sought facilities for the pandits as per united nation norms the idp are those who have been forced to flee their homes but remain within the countrys borders\n",
            "<UNK> <UNK> seeks <UNK> from <UNK>\n",
            "**************************************************\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "1xE8wWsqlTtr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}